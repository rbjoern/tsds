{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Wednesday*, May 2, 2018, 23:59<br>\n",
    "**Peergrading deadline**: *Sunday*, May 6, 2018, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Wednesday*, May 9, 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Questions**](https://github.com/abjer/tsds/issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises will be done using the Marvel Dataset from week 8, and the Sentiment dataset from week 10.\n",
    "\n",
    "***The Sentiment Dataset, is a collection of labeled reviews(positive and negative) from Amazon. The labels are in the column y.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download data\n",
    "url = 'http://www.cs.jhu.edu/~mdredze/datasets/sentiment/domain_sentiment_data.tar.gz'\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "file_tmp = urllib.request.urlretrieve(url, filename=None)[0]\n",
    "base_name = os.path.basename(url)\n",
    "# unzip/untar data\n",
    "file_name, file_extension = os.path.splitext(base_name)\n",
    "tar = tarfile.open(file_tmp)\n",
    "tar.extractall(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data\n",
    "import re\n",
    "import pandas as pd\n",
    "def get_tags(sample_text):\n",
    "    find_tags = '<[^>]+>'\n",
    "\n",
    "    tags = set([i.replace('/','') for i in re.findall(find_tags,sample_text)])\n",
    "    regexes = []\n",
    "\n",
    "    for tag in tags:\n",
    "        pattern = '<\\/?%s>'%tag.strip('<>')\n",
    "        if 'review>' in tag:\n",
    "\n",
    "            linesplitter = re.compile(pattern)\n",
    "            continue\n",
    "        regexes.append(re.compile(pattern))\n",
    "    return regexes,linesplitter\n",
    "def parse_file(filename,domain):\n",
    "    sample_text = open(filename,'r').read(20000)\n",
    "    label = filename.split('/')[-1].split('.')[0]\n",
    "    regexes,linesplitter = get_tags(sample_text)\n",
    "    lines = linesplitter.split(open(filename,'r').read())[1::2]\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        d = {}\n",
    "        d['y'] = label\n",
    "        d['domain'] = domain\n",
    "        for regex in regexes:\n",
    "            col = regex.pattern.split('?')[1][:-1]\n",
    "            val = regex.split(line)[1].strip()\n",
    "            d[col] = val\n",
    "            \n",
    "        data.append(d)\n",
    "    return data\n",
    "\n",
    "from os import listdir\n",
    "def load_sentiment_data():\n",
    "    data = []\n",
    "    base = 'domain_sentiment_data.tar/sorted_data_acl/'\n",
    "    for domain in listdir(base):\n",
    "        path = base+domain\n",
    "        for label in listdir(path):\n",
    "            full_path = path+'/'+label\n",
    "            data+=parse_file(full_path,domain)\n",
    "    return pd.DataFrame(data)\n",
    "sentiment_df = load_sentiment_data()\n",
    "sentiment_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7 - Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.1.1**:\n",
    "* Use regex to mine any numbers mentioned in the reviews.\n",
    "    * Both literal numbers (millions,billions etc) and digits.\n",
    "    * Match numbers before dollar signs $ and percentages %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.1.3**\n",
    "* Lookup how american phone numbers and zipcodes are formatted, and design a regex to capture those.\n",
    "    * Make sure you start broad in your search, so you won't miss the variations.\n",
    "    * Use the zipcodes in conjunction with the **geopy** module to geocode the zipcodes and extract Latitude and Longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.3.1**\n",
    "**Sentiment analysis**\n",
    "* Perform dictionary based sentiment analysis of all the reviews using:\n",
    "    * nltk.corpus.opinion_lexicon, Afinn, nltk.sentiment.SentimentIntensityAnalyzer and TextBlob\n",
    "* Compare the sentiment analysis with the Labels, and see which of the approaches performs best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 8.3.1**: Let's put your classifier to use!\n",
    "* Retrain your model on all of your data.\n",
    "* Use the model to estimate the probability that each character is a villain (let's call this *villainness*). You can use the `.predict_proba` method on the model to get probability estimates rather than class assignments.\n",
    "* **Visualize the \"heroness\" distribution for all ambiguous characters**. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 8.4.1**: Read about [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory).\n",
    "1. What is it? How is it defined mathematically (write out the formula e.g. using LateX formatting)?\n",
    "2. Write a function that computes the Shannon-entropy of a probability vector. Compute the Shannon entropy of `p=[0.4, 0.6]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 8.4.3**: Print the maximum entropy path of a decision tree.\n",
    ">\n",
    ">1. Implement the following pseudocode and print the output:<br><br>\n",
    ">Step 1. Find `team` that gives lowest split entropy for `data`. Print `team`.<br>\n",
    ">Step 2. Split `data` on `team`, to produce `data0` and `data1`. Print the entropy of each, as well as their weighted avg. entropy.<br>\n",
    ">Step 3. Overwrite the `data` variable with either `data0` or `data1`, depending on which has the highest entropy.<br>\n",
    ">Step 4. Stop if there are less than 5 datapoints in `data`. Otherwise start over from 1.<br><br>\n",
    ">My output looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.2.3.1.png) for the first five splits.<br><br>\n",
    ">\n",
    ">2. Comment on decision path your code takes: How splits are there? Do you notice anything interesting about the final splits? Why do we choose to stop splitting before `data` get smaller than 5?\n",
    ">3. Train a `sklearn.tree.DecisionTreeClassifier` classifier on the dataset. Initiate the classifier with `criterion='entropy'`. What are the most important features of this classifier? How does this line up with the order of the order of splits you just printed (a comment is fine)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**ex9.1.1**\n",
    "Make your own implementation of the Naive Bayes classifier following the steps defined by Christopher Manning in his IR book:\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    ">**ex.1.2**\n",
    "Compare your classifier with the sklearn implementation on the sentiment dataset on the following parameters:\n",
    "* Accuracy\n",
    "* Training time using the `%time` or `%timeit` command.\n",
    "\n",
    "**NB**: You don't have to implement it using the sketch of a *class* object provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(object): # a sketch of the class object using sklearn conventions (fit, predict)\n",
    "    def __init__(self,smoothing=1):\n",
    "        # laplace smoothing\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def fit(self,x_train,y_train):\n",
    "        \"\"\"This function will use the training data to estimate p(x_i|C_i) for all features and all categories.\n",
    "        As described by: Christopher Manning in https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\"\"\"\n",
    "    def predict(x_test):\n",
    "        \"This function will do the inference and return the argmax of P(x|C_i). Also described in Mannings book.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Wednesday*, March 21, 2018, 23:59<br>\n",
    "**Peergrading deadline**: *Sunday*, March 25, 2018, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Wednesday*, March 28, 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Questions**](https://github.com/abjer/tsds/issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T09:48:54.997215Z",
     "start_time": "2018-02-07T09:48:54.990521Z"
    }
   },
   "source": [
    ">**Ex 1.1.12**: Just before, you learned about a `dict`, so now you're ready for a `defaultdict`.\n",
    "1. What is a `defaultdict`? How would you say it is different from a normal Python `dict`?\n",
    "2. Write some code that takes a list of tuples:\n",
    "\n",
    ">        l = [(\"a\", 1), (\"b\", 3), (\"a\", None), (\"c\", False), (\"b\", True), (\"a\", None)]\n",
    "\n",
    ">     And produces a `defaultdict` object\n",
    "\n",
    ">        defaultdict(<type 'list'>, {'a': [1, None, None], 'c': [False], 'b': [3, True]})\n",
    "\n",
    ">*Hint: you can import `defaultdict` from `collections`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T09:50:08.398523Z",
     "start_time": "2018-02-07T09:50:08.391787Z"
    }
   },
   "source": [
    ">**Ex 1.1.14**: Take two lists `a = list(\"justreadtheinstructions\")` and `b = list(\"ofcourseistillloveyou\")` and\n",
    "1. get the `set` of characters that exist in both `a` and `b` (intersection),\n",
    "2. get the `set` of characters that exist in either `a` or `b` (union), and\n",
    "3. compute the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) between the distinct elements in `a` and `b`.\n",
    "\n",
    ">*Hint: use the `set` function to get a `set`-type object of distinct elements from a list*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 1.2.2**: Use [`requests`](https://www.google.dk/search?q=python+requests+get+json&gws_rd=cr&ei=M5OdWaewD8Ti6AS54J24Bg), or another Python module, to store **[this data](https://www.reddit.com/r/gameofthrones/.json)** in JSON format in a new variable `data`. Show that the type of `data` is `dict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.2.5**: Each post has a key that indicates whether it is a spoiler or not. Write two `for` loops (or list comprehensions for extra street credits):\n",
    ">1. One that counts the number of spoilers.\n",
    ">2. Another that only prints headlines that aren't spoilers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2\n",
    "\n",
    "Use the code provided in the solutions to obtain dataset `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T13:56:56.798776Z",
     "start_time": "2018-02-06T13:56:56.071801Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "r = requests.get('http://data.kk.dk/dataset/faste-trafiktaellinger')\n",
    "html = r.text\n",
    "\n",
    "re_url = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "urls = re.findall(re_url, html)\n",
    "\n",
    "urls_w_data = [u for u in urls if u.endswith('.xlsx')]\n",
    "\n",
    "data_sets = [pd.read_excel(u, skiprows=10) for u in urls_w_data]\n",
    "\n",
    "data_raw = pd.concat(data_sets)\n",
    "\n",
    "data_idx = data_raw.reset_index(drop=True)\n",
    "data_idx.drop('Spor', axis=1, inplace=True)\n",
    "\n",
    "dk_to_uk = {'Vejnavn':'road_name',\n",
    "            '(UTM32)':'UTM32_east',\n",
    "            '(UTM32).1':'UTM32_north',\n",
    "            'Dato':'date',\n",
    "            'Vej-Id':'road_id'}\n",
    "\n",
    "data_long = data_idx\\\n",
    "                .rename(columns={'index':'year'})\\\n",
    "                .rename(columns=dk_to_uk)\n",
    "        \n",
    "data_long = data_long.copy()\n",
    "\n",
    "del data_sets, data_raw, data_idx        \n",
    "\n",
    "data_long['total'] = data_long.road_id.str[-1] == 'T'\n",
    "\n",
    "data_long = data_long[data_long.total]\n",
    "\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "data_geo = data_long[spatial_columns].copy()\n",
    "\n",
    "data_geo.drop_duplicates(inplace=True)\n",
    "\n",
    "id_columns = ['road_name', 'date']\n",
    "hour_columns = [c for c in data_long.columns if 'kl.' in c]\n",
    "\n",
    "data = pd.melt(data_long, \n",
    "               id_vars=id_columns, \n",
    "               value_vars=hour_columns,\n",
    "               var_name='hour_period',\n",
    "               value_name='traffic')\\\n",
    "         .copy()\n",
    "\n",
    "del data_long\n",
    "\n",
    "data.road_name = data.road_name.astype('category')\n",
    "\n",
    "\n",
    "data['hour'] = data.hour_period.str[3:5]\n",
    "time_str = data.date + '-' + data.hour              \n",
    "data['time'] = pd.to_datetime(time_str, format='%d.%m.%Y-%H')\n",
    "\n",
    "data.drop(['hour', 'date', 'hour_period'], 1, inplace=True)\n",
    "data['week_day'] = data.time.dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.5.2**: Which road has the most average traffic?\n",
    "\n",
    "> *Hint: Start with a `groupby('road_name')` operation on `data`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.5.3**: Compute annual, average road traffic during day hours (9-17). Which station had the least traffic in 2013? Which station has seen highest growth in traffic from 2013 to 2014?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex: 4.2.2**: Create a `networkx.Graph` that represents each partnership between characters, as an edge. Print the number of nodes, edges and average degree of the resulting network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex: 4.2.3**: Plot the degree distribution of your character network. What type of random network does it resemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex: 4.2.4**: Create three bar charts, side-by-side, each with the 10 most high-degree characters one of the classes. Make sure that each subfigure has the same y-axis (control with `plt.ylim` or use `plt.subplots` with parameter `sharey=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex: 4.2.5**: Do the same, but for the highest [betweenness centrality](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html) (BC). Again, make sure the y-axes are shared. When you comment on the result:\n",
    "* Explain briefly what the BC measures (and why it takes some time to compute). An intuitive explanation is also great!\n",
    "* Interpret, if you can, what the differences you see between the distribution of degree and BC means for the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.2**: Implement the modularity function. Write a Python function that takes as input an adjacency matrix and a label vector, and returns the modularity. Compute and print the modularity for the ones given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "])\n",
    "\n",
    "c = [0, 0, 0, 0, 1, 1]\n",
    "\n",
    "def modularity(A, c):\n",
    "    \"\"\"Compute modularity for a labeled network.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "        A : numpy.array\n",
    "            Adjacency matrix. (N, N) square matrix.\n",
    "        c : list of ints\n",
    "            Community labels. Length N.\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "        out : float\n",
    "    \"\"\"\n",
    "    # Your beautiful code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with some real data. Whip out the network you created last week, we will be using that again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 5.1.4**: Remove all nodes that have zero local clustering coefficient. Print the number of nodes and edges in this new graph. Depending on the regex you used to extract the partnerships you may get varying results, but if your network is smaller than ~60 nodes, that's probably not right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.5**: Find the communities in this networks. Visualize it, coloring the nodes according to their label.\n",
    "\n",
    ">*Hint: If you didn't implement the Louvain method in the bonus exercise above, you will need a package which can do it for you. Go ahead and install `python-louvain` by running `conda install -c auto python-louvain` in a terminal. After installation, import it with `import community`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.1 ** Load the municipalities data. \n",
    "- What is the CRS of the GeoDataFrame - what projection does it correspond to? What is the measure of distance? \n",
    "- Which three munipalities have the largest area?\n",
    "\n",
    "> Note: to find the entire area of a municipality which consists of island etc. you can use the `unary_union` method for GeoSeries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.2** Select rows in the GeoDataFrame as follows. Make two boolean series as described below and make a combined series which takes the value True if both holds, otherwise False.\n",
    "> - first: row is True if corresponding the row shape is in the Capital Region or Sealand Region (i.e. `'Region Hovedstaden', 'Region Sjælland'`) \n",
    "> - second: row is True if the  the row geometry is ***not*** in Bornholm or nearby (i.e. `'Bornholm', 'Christiansø'`)\n",
    "\n",
    "> *Hint*: recall that we can check if a series elements are elements in a series using the `isin` method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.3** Extract the extremum values (min,max) in all dimensions.\n",
    "\n",
    "> *Hint*: extreme values, i.e. bounds, can be found using `.bounds` on a GeoDataFrame (also works on shapes, GeoSeries) - you further need to compute the global max, min values of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.4** We are now to construct a 500mx500m grid for the bounds of Sealand and around:\n",
    "- Make a grid of points 500m apart in horizontal and vertical directions that are within the extremum values of Sealand's shape. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.5** Compute interpolation of house price for each grid cell as follows:\n",
    "- Load the pre-structured data with house sales prices for the capital region of Denmark. It is available under the internal course page under files, see [here](https://absalon.instructure.com/courses/24660/files).\n",
    "- Make a loop over sale_year (2012, 2013):\n",
    "    - Fit a nearest neighbor regression model to the square meter price (i.e. `price_area` for each year). Set number of neighbors to 25 and radius to 25000        \n",
    "    - Apply the model to the grid data and assign the estimated price as a column called `pYYYY` where YYYY is the year.\n",
    "    - Make a new column where you store log10 of house price for the given year and call it`pYYYY_log`\n",
    "    \n",
    "> *Hint 1:* sklearn has a regression for k-nearest neighbors\n",
    "\n",
    "> *Hint 2:* use the Easting and Northing in house prices which correspond to the UTM Z32 North CRS, i.e Denmark. In the dataset they are called 'e' and 'n'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 6.3.6:** Visualize the house data\n",
    "- For each of these points construct a square polygon assuming that the point is the south west corner of the square. \n",
    "- Select all sqaures that intersects with Sealand and nearby islands.\n",
    "- Plot the grid data for 2012\n",
    "\n",
    "> *Hint:* Once you have created the grid the following function below may be useful for converting into a GeoDataFrame. You need to specify the column names for your x and y coordinates.\n",
    "\n",
    "> *Hint 2:* We can select the points that intersect by using a spatial join between the house locations and municipalities.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "\n",
    "dk_crs = {'ellps': 'GRS80', 'no_defs': True, 'proj': 'utm', 'units': 'm', 'zone': 32}\n",
    "\n",
    "def cell_coords_to_polygons(square_df, x='e', y='n', dist=500, crs=dk_crs):\n",
    "    '''\n",
    "    Convert coordinates to squares in a GeoDataFrame.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : str\n",
    "        Name of the horizontal coordinate (~longitude)            \n",
    "    y : str\n",
    "        Name of the vertical coordinate (~latitude)                        \n",
    "    dist : int or float\n",
    "        Size of polygons\n",
    "    crs : dict\n",
    "        Coordinate Reference System\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    squares_gdf: geopandas.GeoDataFrame\n",
    "        This table contains squares as geometry\n",
    "        and the original data.\n",
    "    '''\n",
    "    \n",
    "    def _to_square_polygon(row):\n",
    "        '''\n",
    "        This auxiliary function convert a square's lower,left \n",
    "        coordinates to a polygon. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        row : pandas.Series\n",
    "            This is a DataFrame row.            \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        poly: shapely.Polygon        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        d = dist/2\n",
    "        \n",
    "        poly = box(row[x]-d, row[y]-d, row[x]+d, row[y]+d)\n",
    "        \n",
    "        return poly\n",
    "    \n",
    "    # convert to polygons\n",
    "    square_geoms = gpd.GeoSeries(square_df.apply(_to_square_polygon, axis=1), crs=crs)\n",
    "    \n",
    "    # make GeoDataFrame\n",
    "    square_gdf = gpd.GeoDataFrame(data=square_df, geometry=square_geoms)\n",
    "    \n",
    "    return square_gdf\n",
    "\n",
    "# convert example\n",
    "df_example = pd.DataFrame([(617288, 6049782),(617288, 6050282)], columns=['e','n'])\n",
    "gdf_example = cell_coords_to_polygons(df_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

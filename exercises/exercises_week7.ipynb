{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> **TEXT AS DATA**\n",
    "\n",
    "\n",
    "\n",
    "This week we shall be mining the famous **Enron Email Dataset**, that was released after the major scandal.\n",
    "\n",
    "We will combine what we have learned so far about networks with new methods from **information extraction (IE)** and **natural language processing (NLP)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import spacy\n",
    "import afinn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#enron_link ='https://query.data.world/s/boypyv5j5ey55s3mgevys7hbz3halo' \n",
    "#enron_df = pd.read_csv(enron_link)\n",
    "#enron_df.to_csv('enron_data.csv',index=False) \n",
    "enron_df = pd.read_csv('enron_data.csv')\n",
    "enron_df = enron_df[enron_df.content.apply(type)==str] # filter non strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df = enron_df[enron_df.content.apply(type)==str]\n",
    "enron_categories = []\n",
    "for cat in range(1,13):\n",
    "    for level in range(1,3):\n",
    "        category = 'Cat_%d_level_%d'%(cat,level)\n",
    "        if category in enron_df.columns:\n",
    "            \n",
    "            enron_categories.append(category)\n",
    "enron_df = enron_df[[col for col in enron_df.columns if not col in enron_categories and 'weight' not in col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ex. 1.1***\n",
    "* Use regex to mine any number mentioned in the emails.\n",
    "    * Both literal numbers (millions,billions etc) and digits.\n",
    "    * Match currencies $ and percentages %\n",
    "* Rank the people talking most about numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ex. 1.2***\n",
    "* Find names mentioned in emails. - i.e. marked by Capital letters that did not follow punctuation and newlines.\n",
    "    * This is a really hard task, so don't worry that it won't be perfect.\n",
    "    * **NOTE** When parsing both first and surnames, note the differences between a \"lookahead\" for another nam pattern, and a consuming simple pattern. \n",
    "        * i.e. \"John Carl Johnson Jack Jackson\" will be either: [\"John Carl\" \"Johnson Jack\", \"Jacson\"], or [\"John Carl\",\"Carl Johnsen\",\"Johnsen Jack\", \"Jack Johnsen\"]\n",
    "* Which names are mentioned the most, and which people talk most about who?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Ex. 1.3***\n",
    "* Lookup how american phone numbers and zipcodes are formatted, and design a regex to capture those.\n",
    "    * Make sure you start broad in your search, so you won't miss the variations.\n",
    "    * Use the zipcodes in conjunction with the **geopy** module to geocode the zipcodes and extract Latitude and Longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Ex. 1.4***\n",
    "* Extract all email addresses and links from the content column.\n",
    "\n",
    "***Ex. 1.5*** -- extras\n",
    "    * Extract dates - and \"in-the-wild\" coordinations: e.g. on Friday at 20 o'clock.\n",
    "    * Extract emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis and data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting acquinted with the Enron dataset.**\n",
    "\n",
    "*** ex.2.1 *** \n",
    "\n",
    "* Do an basic exploratory analysis of the dataset.\n",
    "    * Plot basic distributions: \n",
    "        * e.g. how many different users\n",
    "        * Activity over time (daily,weekly, monthly).\n",
    "            * Of different users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with network statistics\n",
    "\n",
    "*** ex.2.2 ***\n",
    "* Construct a directed network from the columns ['X-From','X-To','X-cc','X-bcc'] (at least the first two).\n",
    "    * Make sure you add the index of the email to the edge metadata for later integration.\n",
    "    * Make sure you parse the To columns so they match the X-From."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*** ex. 2.3 ***\n",
    "** Explore the network ** \n",
    "* How many different edges are present in the data over time?\n",
    "* Plot a suitably sized subgraph using the K-core algorithm for extracting central components.\n",
    "* Extract **central actors** according to a metric of choice save these for later investigations.\n",
    "* Make sets of **important edges**, according to three principles:\n",
    "    * **Highly active** edges.\n",
    "    * They have **bridging** qualities (e.g. edge-betweeness-centrality).\n",
    "    * They are **Clustered**: e.g. has a high overlap of neighbors, or the strongest edges within a community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Characterizing actors and edges based on the content of the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Exercise 2.4 ***\n",
    "\n",
    "First we should get acquinted with one of the exploratory tools: the wordcloud.\n",
    "\n",
    "** Wordclouds ** Install the module [wordcloud](https://github.com/amueller/word_cloud): conda install -c conda-forge wordcloud\n",
    "`from wordcloud import Wordcloud`\n",
    "\n",
    "Look in the documentation on how to construct wordclouds.\n",
    "* Make wordclouds of the top 5 people ranked by a network measure you like (e.g. in-degree):\n",
    "    * Aggregate words statistics from:\n",
    "        * The emails they write\n",
    "        * The emails they receive, and\n",
    "        * **Extra** the difference between the two.\n",
    "            * **note** Think about how to calculate a distance between word distributions. (Look up TF-IDF)\n",
    "            \n",
    "    * **note** this takes a lot of iterations, designing filters and stopwords. Take departure in the normalize_token function defined during todays lectures.\n",
    "    \n",
    "       \n",
    "**Extra** \n",
    "    * Aggregate word statistics from sets of *Important Edges* and compare the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) # remember to define as set for performance\n",
    "#The stemmers and lemmers need to be initialized before being run\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "wordnet = nltk.stem.WordNetLemmatizer()\n",
    "def normalize_tokens(tokens,lowercase=False\n",
    "                     ,remove_non_alpha=False\n",
    "                     , stop_words = False\n",
    "                     , stemmer = False\n",
    "                     , lemmer = False):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    workingIter = tokens\n",
    "    #removing non-words\n",
    "    if remove_non_alpha:\n",
    "        workingIter = (w for w in workingIter if w.isalpha())\n",
    "    \n",
    "    # lowering\n",
    "    if lowercase:\n",
    "        workingIter = (w.lower() for w in workingIter)\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "        \n",
    "    #And the lemmer\n",
    "    if lemmer:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "    \n",
    "    #And remove the stopwords\n",
    "    if stop_words:\n",
    "        workingIter = (w for w in workingIter if w not in stop_words)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ex. 3.1 ***\n",
    "**Sentiment analysis**\n",
    "* Perform a sentiment analysis to characterize edges. \n",
    "* Plot sentiment score against a measure of edge importance as described above(betweeness, activity levels, clustering).\n",
    "    * **Extra:** calculate a relative sentiment score of each edge, in relation to the average sentiment of each node.\n",
    "\n",
    "** extra **\n",
    "* See if the results differ when using different sentiment analysis methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Ex. 3.2 *** \n",
    "** POS-tagging **\n",
    "* Investigate the use of different word classes. \n",
    "    * e.g. what comes after possitve pronouns ('PRP$': e.g. 'my', 'mine') and possessive endings (POS:e.g. \"'\")\n",
    "* Characterize edges by the use of adjectives and verbs\n",
    "    * E.g. a ratio of verbs/adjectives.\n",
    "\n",
    "**extra**: Create a co-occurence network between verbs and adjectivesin the same email or sentence (or context-window), and adjectives. Locate clusters of verbs and adjectives.\n",
    "\n",
    "*** Ex. 3.3 ***\n",
    "**NER-tagging**\n",
    "* Extract entities from emails.\n",
    "* What organizations are important in this company?\n",
    "\n",
    "** Combine the sentiment analysis with the entities located.**\n",
    "    * Tokenize sentences, and attribute the sentiment of a sentence to the Entity present in the sentiment.\n",
    "    * Can you locate any malicious gossip?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Apollo Beth',\n",
       "   'Beck Sally',\n",
       "   'Becker Melissa',\n",
       "   'Belden Tim',\n",
       "   'Black Don',\n",
       "   'Bradford William S.',\n",
       "   'Bryan Jennifer',\n",
       "   'Buy Rick',\n",
       "   'Carrizales Blanca',\n",
       "   'Causey Richard',\n",
       "   'Colwell Wes',\n",
       "   'Dayao Anthony',\n",
       "   'Dietrich Janet',\n",
       "   'Heathman Karen K.',\n",
       "   'Hinojosa Esmeralda',\n",
       "   'Holmes Sean',\n",
       "   'Leff Dan',\n",
       "   'Presto Kevin M.',\n",
       "   'Stubblefield Wade',\n",
       "   'Tijerina Shirley',\n",
       "   'Wadsworth Sue'],\n",
       "  'Apollo, Beth </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Bapollo>, Beck, Sally </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Sbeck>, Becker, Melissa </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Mbecker>, Belden, Tim </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Tbelden>, Black, Don </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Dblack>, Bradford, William S. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Wbradfo>, Bryan, Jennifer </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c4b2bc0b-5c606c03-86256849-57b86a>, Buy, Rick </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Rbuy>, Carrizales, Blanca </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Bcarriz>, Causey, Richard </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Rcausey>, Colwell, Wes </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Wcolwel>, Dayao, Anthony </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Adayao>, Dietrich, Janet </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=384eca1e-36846ef5-62569fb-57dcf1>, Heathman, Karen K. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Kheathm>, Hinojosa, Esmeralda </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=b51e944e-5c32fb62-862566e3-5fde1f>, Holmes, Sean </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=73a4bd83-d6dd722c-862564e4-6ac227>, Leff, Dan </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=9bb28634-f7b2062d-862564de-77878a>, Presto, Kevin M. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Kpresto>, Stubblefield, Wade </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=4dad4484-cce8dee5-862567d2-7524a8>, Tijerina, Shirley </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Stijeri>, Wadsworth, Sue </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Swadswo>'),\n",
       " (['<mba01@haas.berkeley.edu>'],\n",
       "  '<mba01@haas.berkeley.edu>, <mba02@haas.berkeley.edu>, <evmba_social@haas.berkeley.edu>'),\n",
       " (['Jeff Dasovich', 'Dasovich/NA/Enron@Enron>'],\n",
       "  'Jeff Dasovich <Jeff Dasovich/NA/Enron@Enron>'),\n",
       " (['Young Steve'], 'Young, Steve </O=ENRON/OU=NA/CN=RECIPIENTS/CN=SYoung>'),\n",
       " (['<rich.heidorn@ipgdirect.com>',\n",
       "   'Suzanna Strangmeier',\n",
       "   'Rachel Steffens',\n",
       "   'Jeff Maack',\n",
       "   'Mark Nutter',\n",
       "   'Gregg Daileda'],\n",
       "  '<rich.heidorn@ipgdirect.com>, <ellen.clardy@ipgdirect.com>, \"Suzanna Strangmeier\" <suzanna.strangmeier@ipgdirect.com>, \"Rachel Steffens\" <rachel.steffens@ipgdirect.com>, \"Jeff Maack\" <jeff.maack@ipgdirect.com>, \"Mark Nutter\" <mark.nutter@ipgdirect.com>, \"Gregg Daileda\" <Gregg.Daileda@ipgdirect.com>'),\n",
       " (['Lexi Elliott'], 'Lexi Elliott'),\n",
       " (['Sally Beck'], 'Sally Beck'),\n",
       " (['Tim Belden'], 'Tim Belden'),\n",
       " (['Rogers Rex',\n",
       "   'Davis Hardie',\n",
       "   'Causey Richard',\n",
       "   'Butcher Sharon',\n",
       "   'DeSpain Tim',\n",
       "   'Parsons Andrew',\n",
       "   'Carson Rick L.'],\n",
       "  'Rogers, Rex </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Rroger2>, Davis, Hardie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Hdavis>, Causey, Richard </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Rcausey>, Butcher, Sharon </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Sbutche>, DeSpain, Tim </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Tdespai>, Parsons, Andrew </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Aparson>, Carson, Rick L. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Rcarson>'),\n",
       " (['Cheatsheets Mailing List'],\n",
       "  '\"Cheatsheets Mailing List\" <cheatsheets@egroups.com>')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_To_names(val): # My ugly implemntation\n",
    "    if type(val)!=str:\n",
    "        return None\n",
    "    names = []\n",
    "    current_name = []\n",
    "\n",
    "    opposite_name = False # for handling names like Anderson, John. Instead of John Anderson\n",
    "    for name in val.split():\n",
    "        # two names\n",
    "        if ',' in name:\n",
    "            if len(current_name)==0:\n",
    "                opposite_name = True\n",
    "                current_name.append(name.strip(','))\n",
    "            else:\n",
    "                if opposite_name:\n",
    "                    names.append(' '.join(current_name))\n",
    "                else:\n",
    "                    names.append(' '.join(reversed(current_name)))\n",
    "                current_name = []\n",
    "                opposite_name = False\n",
    "        elif name[0]!='<':\n",
    "            current_name.append(name)\n",
    "        else:\n",
    "            names.append(' '.join(current_name))\n",
    "            current_name = []\n",
    "    if len(current_name)>0:\n",
    "        names.append(' '.join(current_name))\n",
    "    return [i for i in list(map(lambda x: x.strip('\"\\' '),names)) if len(i)!=0]\n",
    "sample = enron_df.sample(10)\n",
    "list(zip(sample['X-To'].apply(get_To_names),sample['X-To'])) # try doing a regex based one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "124px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

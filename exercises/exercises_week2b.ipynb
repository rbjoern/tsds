{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Every week, you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NOT EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `tsds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Week 2b: Data structuring\n",
    "\n",
    "*Thursday, February 15, 2018*\n",
    "\n",
    "In this part of today's session you will be working with traffic data from Copenhagen Municipality. Note that this part is quite long. The reason is that there is a lot of catching up and recap from our summer course.\n",
    "\n",
    "The municipality have made the data openly available through the `opendata.dk` platform. We will use the data from [traffic counters](http://data.kk.dk/dataset/faste-trafiktaellinger) to construct a dataset of hourly traffic. We will use this data to get basic insights on the development in traffic over time and relate it to weather. The gist here is to practice a very important skill in Data Science: being able to quickly fetch data from the web and structure it so that you can work with it. Scraping usually gets a bit more advanced than what we will do today, but the following exercises should give you a taste for how it works. The bulk of these exercise, however, revolve around using the *Pandas* library to structure and analyze data.\n",
    "\n",
    "An overview of today's exercises:\n",
    "* 2b.1: Get some traffic data\n",
    "* 2b.2: Structure your dataset\n",
    "* 2b.3: String data, selection and rotation\n",
    "* 2b.4: Structure temporal data\n",
    "* 2b.5: Statistical descriptions of traffic data\n",
    "* 2b.6 (extra): Working with weather station data from NOAA\n",
    "* 2b.7 (extra): Further learning\n",
    "\n",
    "*Note for R-users*: Pandas is a lot like *R* so if you are a shark at that there's no need relearn [things you already know](https://pandas.pydata.org/pandas-docs/stable/comparison_with_r.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.1: Get some traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence follows a simple scraping exercise where you (1) collect urls for datasets in the webpage listing data on [traffic counters](http://data.kk.dk/dataset/faste-trafiktaellinger) and (2) use these urls to load the data into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape dataset urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.1.1**: Using the `requests` module, extract the html markup of the webpage and store it as a string in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.1.2**: Using the `re` module, extract a list of all the urls in the html string and store them in a new variable.\n",
    "\n",
    "> *Hint: Try using the `re.findall` method. You may want to Google around to figure out how to do this. Protip: searching for something along the lines of \"extract all links in html regex python\" and hitting the first StackOverflow link will probably get you farther than reading elaborate documentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.1.3**: Create a new variable that only contains the links to traffic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.1.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load everything into a single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.1.4**: Using `pd.read_excel` method, load the datasets into a list. Your resulting variable should hold a list of Pandas dataframes.\n",
    "\n",
    "> *Note: you may want to set the `skiprows=` keyword argument.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.1.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.1.5**: Merge the list of dataframes into a single dataframe.\n",
    "\n",
    "> *Hint: try using pandas' `concat` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.2: Structure your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you successfully completed the previous part, you should now have a dataframe with about 183.397 rows (if your number of rows is close but not the same, worry not—it matters little in the following). Well done! But the data is still in no shape for analysis, so we must clean it up a little.\n",
    "\n",
    "183.397 rows (and 30 columns) is a lot of data. ~3.3 MB by my back-of-the-envelope calculations, so not \"Big Data\", but still enough to make your CPU heat up if you don't use it carefully. Pandas is built to handle fairly large dataframes and has advanced functionality to perform very fast operations even when the size of your data grows huge. So instead of working with basic Python we recommend working `pandas` built-in procedures as they are constructed to be fast on dataframes.\n",
    "\n",
    "*Nerd fact: the reason `pandas` is much faster than pure Python is that dataframes access a lower level programming languages (namely C, C++) which are multiple times faster than Python. The reason it is faster is that it has a higher level of explicitness and thus is more difficult to learn and navigate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidy indices and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember `numpy` arrays from last week? Unlike these, `pandas` dataframes have the advantage that columns and rows can be labeled. These labels are referred to respectively as *row indices* and *column names*. We start out with formatting the indices and altering the column names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.2.1**: Reset the row indices of your dataframe so the first index is 0 and the last is whatever the number of rows your dataframe has.\n",
    "\n",
    "> *Hint: Check out the `reset_index` method for dataframes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.2.2**: The column called `Spor` is superfluous. Delete it.\n",
    "\n",
    "> *Hint: try using the `drop` method. What does keyword arguments `inplace=`, `axis=` do?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.2.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.2.3**: Rename variables from Danish to English using the dictionary below.\n",
    "\n",
    "> *Hint: this is possible using the dataframes' `rename` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:21:47.092955Z",
     "start_time": "2018-02-06T14:21:47.087050Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.2.3]\n",
    "\n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mind your memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is quite efficient. For example, when you create a new dataframe by manipulating an old one, Python notices that—apart from some minor changes—these two objects are almost the same. Since memory is a precious resource, Python will represent the values in the new dataframe as *references* to the variables in the old dataset. This is great for performance, but if you for whatever reason change some of the values in your old dataframe, values in the new one will also change—and we don't want that! Luckily, we can break this dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.2.4**: Break the dependencies of the dataframe that resulted from Ex. 2b.2.3. Delete all other dataframes. \n",
    "\n",
    "> *Hint: try using the dataframes' `copy` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.2.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.3: String data, selection and rotation\n",
    "Once you have structured appropriately, something that you will want to do again and again is **selecting subsets of the data**. Specifically, it means that you select specific rows in the dataset based on some column values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic operations: selecting and subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.3.1**: Create a new column in the dataframe called `total` that is `True` when the last letter of `road_id` is T and otherwise `False`.\n",
    "\n",
    "> *Hint1: try using `str` method for pandas series/columns for accessing the string elements, e.g. \"data.road_id.str[2]\".*\n",
    "\n",
    "> *Hint2: you can use the equal operator `==` on series/columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.3.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.3.2** Select rows where `total` is True. Delete all the remaining observations.\n",
    "\n",
    "> *Hint: try to get inspiration from this [Stack Overflow question](https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.3.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.3.3**: Make two datasets based on the lists of columns below. Call the dataset with spatial columns `data_geo` and the other `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T15:07:38.730247Z",
     "start_time": "2018-02-06T15:07:38.725303Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.3.3]\n",
    "\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = ['kl.%s-%s' % (str(h).zfill(2), str(h+1).zfill(2)) for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.3.4**: Drop the duplicate rows in `data_geo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.3.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting: wide and narrow format\n",
    "\n",
    "When talking about two-dimensional data (matrices, tables or dataframes, we can call it many things), we can either say that it is in *wide* or *long* format (see explanation [here](https://en.wikipedia.org/wiki/Wide_and_narrow_data), \"wide\" and \"long\" are used interchangably). In Pandas we can use the commands `stack` and `unstack` to move between these formats.\n",
    "\n",
    "The wide format has the advantage that it often requires less storage and is easier to read when printed. On the other hand the long format can be easier for modelling, because each observation has its own row. Turns out that the latter is what we most often need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.3.5**: Turn the dataset from wide to long so hourly data is now vertically stacked. Store this dataset in a dataframe called `data`. Name the column with hourly information `hour_period`. Your resulting dataframe should look something like [this](http://ulfaslak.com/tsds/ex_235_example.png).\n",
    "\n",
    "> *Hint: pandas' `melt` function may be of use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.3.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical data\n",
    "\n",
    "Categorical data can contain Python objects, usually strings. These are smart if you have variables with string observations that are long and often repeated, e.g. with road names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.3.6**: Convert the *type* of the `road_name` column to categorical.\n",
    "\n",
    "> *Hint: The method `astype` for series/columns may be of use.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.3.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.4: Structure temporal data\n",
    "\n",
    "Pandas has native support for working with temporal data. This is handy as much 'big data' often has time stamps which we can make Pandas aware of. Once we have encoded temporal data it can be used to extract information such as the hour, second etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.4.1**: Create a new column called `hour` which contains the hour-of-day for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.4.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.4.2**: Create a new column called `time`, that contains the time of the row in `datetime` format. Delete the old temporal columns (hour, hour_period, date) to save memory.\n",
    "\n",
    "> *Hint: try making an intermediary series of strings that has all temporal information for the row; then use pandas `to_datetime` function where you can specify the format of the date string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.4.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.4.3**: Using your `time` column make a new column called `weekday` which stores the weekday (in values between 0 and 6) of the corresponding `datetime`.\n",
    "\n",
    "> *Hint: try using the `dt` method for the series called `time`; `dt` has some relevant methods itself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.4.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.4.4**: What other things can `dt` be used to compute? Try to compute week- and month number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.4.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.5: Statistical descriptions of traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.5.1**: Print the \"descriptive statistics\" of the `traffic` column.\n",
    "\n",
    "> *Hint: Use the `describe` method of pandas dataframes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.5.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.5.2**: Which road has the most average traffic?\n",
    "\n",
    "> *Hint: Start with a `groupby('road_name')` operation on `data`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.5.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.5.3**: Compute annual, average road traffic during day hours (9-17). Which station had the least traffic in 2013? Which station has seen highest growth in traffic from 2013 to 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.5.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additioal exercises and further learning\n",
    "\n",
    "This final exercise is an old exercise from our summer course that we recommend that you finish. It has an exercise of joining different datasets into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.6: Working with weather station data from NOAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2b.6.1**: Do the in class exercises from the SDS course [here](https://abjer.github.io/sds/slides/in_class_exercise.ipynb). Note that the solution is available in the lecture [slides](https://abjer.github.io/sds/slides/plotting.pdf)/[notebook](https://abjer.github.io/sds/slides/plotting.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2b.6.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b.7: Further learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many important topics for DataFrames have been skipped. These include:\n",
    "\n",
    "- Copying data in python: deep vs. shallow - `copy` method for dataframes   \n",
    "- Working with duplicates: dataframe methods `duplicated`, `drop_duplicates`\n",
    "- Working with timeseries methods for dataframe e.g. `diff`, `shift`, `resample`, `rolling`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as always: **do not edit in tsds folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, reuters\n",
    "# List of document ids\n",
    "documents = reuters.fileids()\n",
    " \n",
    "# Load training data and test data\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),\n",
    "                            documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),\n",
    "                           documents))\n",
    "# Load categories\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "categories = reuters.categories()\n",
    "cat2index = {cat:num for num, cat in enumerate(categories)}\n",
    "def process_categories(cats):\n",
    "    \"This function takes a list of named categories and converts them into a sparse indicator vector.\"\n",
    "    vec = np.zeros(len(cat2index))\n",
    "    for cat in cats:\n",
    "        idx = cat2index[cat]\n",
    "        vec[idx] = 1\n",
    "    return vec\n",
    "def make_y(fileids):\n",
    "    y_mat = []\n",
    "    for doc in fileids:\n",
    "        y_mat.append(process_categories(reuters.categories(doc)))\n",
    "    return pd.DataFrame(y_mat,columns=categories)\n",
    "y_df_train = make_y(train_docs_id)\n",
    "y_df_test = make_y(test_docs_id)\n",
    "# Documents\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "# lemmatizing\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(words):\n",
    "    pos_doc = nltk.pos_tag(words)\n",
    "    pos_doc = [(word,get_wordnet_pos(tag)) for word,tag in pos_doc]\n",
    "    return [lemmatizer.lemmatize(word,tag) if tag!='' else word for word,tag in pos_doc]\n",
    "#### NORMALIZATION+TOkenization\n",
    "# Define stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_nltk = set(stopwords.words('english')) # remember to define as set for performance\n",
    "#own_stop_words = set([\"the\",\"it\",\"she\",\"he\", \"a\",\"trump\",'idiot','failure','weak']) # Define your own list of stopwords\n",
    "\n",
    "#The stemmers and lemmers need to be initialized before being run\n",
    "porter = nltk.stem.porter.PorterStemmer() # \n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english') # a greedy stripper\n",
    "# Normalization using a Class \n",
    "class custom_tokenizer():\n",
    "    def __init__(self,lowercase=False\n",
    "                     ,remove_non_alpha=False\n",
    "                     , stopwords = False\n",
    "                     , stemmer = False\n",
    "                     , lemmer = False,\n",
    "                     tokenizer = nltk.word_tokenize):\n",
    "        self.lowercase = lowercase\n",
    "        self.tokenizer = tokenizer\n",
    "        self.remove_non_alpha = remove_non_alpha\n",
    "        self.lemmer = lemmer\n",
    "        self.stemmer = stemmer\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "    def tokenize(self,string):\n",
    "        ####### Optional ######\n",
    "        # Filter unwanted elements, or substitute elements using regex here.\n",
    "        ## Examples: \n",
    "        ### different URLS substituted for one __link__ token\n",
    "        ### numbers substituted __number__ token, or separate tokens __zipcode__, __phonenumber__, and __money__.\n",
    "\n",
    "        # preprocess clean strings.\n",
    "        ####### Optional ######\n",
    "\n",
    "        # Split strings into words.\n",
    "        words = self.tokenizer(string)\n",
    "        # Lemmatizing\n",
    "        if self.lemmer:\n",
    "            words = lemmatize(words)\n",
    "        #removing non-words\n",
    "        if self.remove_non_alpha:\n",
    "            words = [w for w in words if w.isalpha()]\n",
    "\n",
    "        # lowering\n",
    "        if self.lowercase:\n",
    "            words = [w.lower() for w in words]\n",
    "        #Now we can use the semmer, if provided\n",
    "        if self.stemmer:\n",
    "            words = [self.stemmer.stem(w) for w in words]\n",
    "\n",
    "        #And remove the stopwords\n",
    "        if self.stopwords:\n",
    "            words = [w for w in words if w not in self.stopwords]\n",
    "        #We will return a list with the stopwords removed\n",
    "        return words\n",
    "\n",
    "\n",
    "tokenizer = custom_tokenizer(**default) # change configurations here\n",
    "gram = 2 # define number of ngrams\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,gram+1),min_df=3,tokenizer=tokenizer.tokenize)\n",
    "# load documents\n",
    "vectorizer.fit(train_docs)\n",
    "mat_train = vectorizer.transform(train_docs)\n",
    "mat_test = vectorizer.transform(test_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Implementation of the Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex.1.1**\n",
    "Make your own implementation of the Naive Bayes classifier following the steps defined by Christopher Manning in his IR book:\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "**ex.1.2**\n",
    "Compare your classifier with the sklearn implementation on the reuters dataset on the following parameters:\n",
    "* Accuracy\n",
    "* Training time using the `%timeit` command.\n",
    "\n",
    "***ex.1.3***\n",
    "Install the code profilling tool: line_profiler\n",
    "     `conda install line_profiler`\n",
    "     \n",
    "And use this tool to profile your own classifier:\n",
    "\n",
    "`%load_ext line_profiler\n",
    "%lprun -f NB.fit NB.fit(x_train,y_train)`\n",
    "\n",
    "***extra***\n",
    "Optimize your implementation. ***Hint**: Can you create more matrix operations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB(object): \n",
    "    def __init__(self,smoothing=1):\n",
    "        # laplace smoothing\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def fit(self,x_train,y_train):\n",
    "        \"\"\"This function will use the training data to estimate p(x_i|C_i) for all features and all categories.\n",
    "        As described by: Christopher Manning in https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\"\"\"\n",
    "    def predict(x_test):\n",
    "        \"This function will do the inference and return the argmax of P(x|C_i). Also described in Mannings book.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze test results\n",
    "We are gonna use our simulation results to learn about each classifier.\n",
    "**ex.2.1**:\n",
    "    - Plot the learning curve of each classifier.\n",
    "        - Broken down on each task.\n",
    "        - Are the classifiers saturated or do they need more samples?\n",
    "        - Does it vary with the vocab_size?\n",
    "\n",
    "**ex.2.2**\n",
    "    - Investigate inference time for each classifier in relation to training_size and vocab_size.\n",
    "\n",
    "**ex.2.3**:\n",
    "    - Investigate training_time for each classifier in relation to the vocab_size.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>category</th>\n",
       "      <th>clf</th>\n",
       "      <th>f1</th>\n",
       "      <th>infer_time</th>\n",
       "      <th>ngram</th>\n",
       "      <th>precision</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>recall</th>\n",
       "      <th>train_time</th>\n",
       "      <th>training_size</th>\n",
       "      <th>vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.934415</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>nb</td>\n",
       "      <td>0.331081</td>\n",
       "      <td>0.038709</td>\n",
       "      <td>2</td>\n",
       "      <td>0.418803</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true, \"lemmer...</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.048521</td>\n",
       "      <td>3500</td>\n",
       "      <td>50995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.953958</td>\n",
       "      <td>acq</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.898909</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>2</td>\n",
       "      <td>0.942073</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true, \"remove...</td>\n",
       "      <td>0.859527</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>500</td>\n",
       "      <td>38388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.984432</td>\n",
       "      <td>earn</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.978470</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974453</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true, \"lemmer...</td>\n",
       "      <td>0.982521</td>\n",
       "      <td>0.178637</td>\n",
       "      <td>4500</td>\n",
       "      <td>11090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>0.932428</td>\n",
       "      <td>earn</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.910839</td>\n",
       "      <td>1.407339</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867610</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true, \"remove...</td>\n",
       "      <td>0.958602</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>5000</td>\n",
       "      <td>9751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.970520</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.729483</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>2</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true}</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.259074</td>\n",
       "      <td>2500</td>\n",
       "      <td>51365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.794634</td>\n",
       "      <td>earn</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.770880</td>\n",
       "      <td>1.062056</td>\n",
       "      <td>2</td>\n",
       "      <td>0.644225</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true}</td>\n",
       "      <td>0.959522</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>2500</td>\n",
       "      <td>51365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.988076</td>\n",
       "      <td>earn</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.983395</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>2</td>\n",
       "      <td>0.986124</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true, \"stemme...</td>\n",
       "      <td>0.980681</td>\n",
       "      <td>0.499916</td>\n",
       "      <td>4000</td>\n",
       "      <td>51169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.983107</td>\n",
       "      <td>earn</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.976616</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.973492</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true, \"lemmer...</td>\n",
       "      <td>0.979761</td>\n",
       "      <td>0.350015</td>\n",
       "      <td>4000</td>\n",
       "      <td>11090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.982445</td>\n",
       "      <td>earn</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.975655</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974312</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true}</td>\n",
       "      <td>0.977001</td>\n",
       "      <td>0.028363</td>\n",
       "      <td>2000</td>\n",
       "      <td>13131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.934084</td>\n",
       "      <td>money-fx</td>\n",
       "      <td>nb</td>\n",
       "      <td>0.364217</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>2</td>\n",
       "      <td>0.425373</td>\n",
       "      <td>{\"lowercase\": true, \"stopwords\": true}</td>\n",
       "      <td>0.318436</td>\n",
       "      <td>0.028289</td>\n",
       "      <td>4000</td>\n",
       "      <td>51365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy  category  clf        f1  infer_time  ngram  precision  \\\n",
       "836   0.934415  money-fx   nb  0.331081    0.038709      2   0.418803   \n",
       "1050  0.953958       acq  svm  0.898909    0.002929      2   0.942073   \n",
       "624   0.984432      earn  svm  0.978470    0.002865      1   0.974453   \n",
       "1021  0.932428      earn  knn  0.910839    1.407339      1   0.867610   \n",
       "186   0.970520  money-fx  svm  0.729483    0.003352      2   0.800000   \n",
       "266   0.794634      earn  knn  0.770880    1.062056      2   0.644225   \n",
       "473   0.988076      earn  svm  0.983395    0.004324      2   0.986124   \n",
       "623   0.983107      earn  svm  0.976616    0.002128      1   0.973492   \n",
       "19    0.982445      earn  svm  0.975655    0.001375      1   0.974312   \n",
       "237   0.934084  money-fx   nb  0.364217    0.015915      2   0.425373   \n",
       "\n",
       "                                          preprocessing    recall  train_time  \\\n",
       "836   {\"lowercase\": true, \"stopwords\": true, \"lemmer...  0.273743    0.048521   \n",
       "1050  {\"lowercase\": true, \"stopwords\": true, \"remove...  0.859527    0.007315   \n",
       "624   {\"lowercase\": true, \"stopwords\": true, \"lemmer...  0.982521    0.178637   \n",
       "1021  {\"lowercase\": true, \"stopwords\": true, \"remove...  0.958602    0.004258   \n",
       "186              {\"lowercase\": true, \"stopwords\": true}  0.670391    0.259074   \n",
       "266              {\"lowercase\": true, \"stopwords\": true}  0.959522    0.004228   \n",
       "473   {\"lowercase\": true, \"stopwords\": true, \"stemme...  0.980681    0.499916   \n",
       "623   {\"lowercase\": true, \"stopwords\": true, \"lemmer...  0.979761    0.350015   \n",
       "19               {\"lowercase\": true, \"stopwords\": true}  0.977001    0.028363   \n",
       "237              {\"lowercase\": true, \"stopwords\": true}  0.318436    0.028289   \n",
       "\n",
       "      training_size  vocab_size  \n",
       "836            3500       50995  \n",
       "1050            500       38388  \n",
       "624            4500       11090  \n",
       "1021           5000        9751  \n",
       "186            2500       51365  \n",
       "266            2500       51365  \n",
       "473            4000       51169  \n",
       "623            4000       11090  \n",
       "19             2000       13131  \n",
       "237            4000       51365  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('test_results2.csv')\n",
    "test_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# First we will rename the preprocessing column\n",
    "def rename_preprocessing(js):\n",
    "    \"This function will remove the default settings, that was not experimented with\"\n",
    "    js = json.loads(js)\n",
    "    del js['stopwords']\n",
    "    del js['lowercase']\n",
    "    return json.dumps(js).strip('{} ')\n",
    "test_df['preprocessing'] = test_df.preprocessing.apply(rename_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization of large datasets\n",
    "When working with large datasets vectorization cannot be done with all data loaded into memory at once. An optimal indexing (i.e. the most prevalent words are assigned to smaller indexes) is therefore inefficient since it would require means running over the data twice. Instead one can transform each input and index on the fly.\n",
    "\n",
    "**ex.3.1** In this exercise I want you to implement a vectorization scheme as described here: https://cmry.github.io/notes/ngrams\n",
    "\n",
    "**ex.3.2** Apply this function to the Reuters data, but instead of loading it all at the same time I want you to design an interator that reads each sample from disk and applies your new vectorization scheme.\n",
    "\n",
    "***extra*** \n",
    "For each 1000 samples, I want you to do online training of a classifier using the partial_fit function of a sklearn classifier: e.g. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.partial_fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "68px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

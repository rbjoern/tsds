{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**as always.. do not edit this in your tsds folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises for today you are going to experiment with what performance gains Semi-Supervised learning, and Transfer Learning can give you.\n",
    "\n",
    "**Dataset**\n",
    "It is always good to test the your models and assumption on different tasks, therefore I provide you with another labeled dataset (sentiment Amazon Review dataset - Multi domain - i.e. different review categories), and the code to load it. However you are free to choice which one you like, but if you want to test the gains from the embedding approach on the sentiment data, you might have to train a new doc2vec model on the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "url = 'http://www.cs.jhu.edu/~mdredze/datasets/sentiment/domain_sentiment_data.tar.gz'\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "file_tmp = urllib.request.urlretrieve(url, filename=None)[0]\n",
    "base_name = os.path.basename(url)\n",
    "# unzip/untar data\n",
    "file_name, file_extension = os.path.splitext(base_name)\n",
    "tar = tarfile.open(file_tmp)\n",
    "tar.extractall(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_tags(sample_text):\n",
    "    find_tags = '<[^>]+>'\n",
    "\n",
    "    tags = set([i.replace('/','') for i in re.findall(find_tags,sample_text)])\n",
    "    regexes = []\n",
    "\n",
    "    for tag in tags:\n",
    "        pattern = '<\\/?%s>'%tag.strip('<>')\n",
    "        if 'review>' in tag:\n",
    "\n",
    "            linesplitter = re.compile(pattern)\n",
    "            continue\n",
    "        regexes.append(re.compile(pattern))\n",
    "    return regexes,linesplitter\n",
    "def parse_file(filename,domain):\n",
    "    sample_text = open(filename,'r').read(20000)\n",
    "    label = filename.split('/')[-1].split('.')[0]\n",
    "    regexes,linesplitter = get_tags(sample_text)\n",
    "    lines = linesplitter.split(open(filename,'r').read())[1::2]\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        d = {}\n",
    "        for regex in regexes:\n",
    "            col = regex.pattern.split('?')[1][:-1]\n",
    "            val = regex.split(line)[1].strip()\n",
    "            d[col] = val\n",
    "            d['y'] = label\n",
    "            d['domain'] = domain\n",
    "        data.append(d)\n",
    "    return data\n",
    "\n",
    "from os import listdir\n",
    "def load_sentiment_data():\n",
    "    data = []\n",
    "    base = 'domain_sentiment_data.tar/sorted_data_acl/'\n",
    "    for domain in listdir(base):\n",
    "        path = base+domain\n",
    "        for label in listdir(path):\n",
    "            full_path = path+'/'+label\n",
    "            data+=parse_file(full_path,domain)\n",
    "    return pd.DataFrame(data)\n",
    "sentiment_df = load_sentiment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised learning\n",
    "Here you will implement and test the performance of a simple semisupervised approach.\n",
    "\n",
    "* First you choose a classifier (e.g. naive bayes, svm or the one of the Doc2Vec transformations), only thing is it needs to have the function predict_proba.\n",
    "\n",
    "* Then you define the minimum Certainty threshold you want for your predicted training labels. The training stops when no new predictions is above the certainty threshold.\n",
    "\n",
    "* Then you define the number of new labels pr. iteration - this essentially defines the overall training time, in that it defines the number of training iterations. \n",
    "\n",
    "* Define a Fit function, that iterates between fitting a new model and getting new training labels until (i.e. while) there are no more unlabeled labels or  no more labels with high certainty to be obtained.\n",
    "\n",
    "**Ex1.x** \n",
    "* Experiment with the certainty threshold and number of new predicted samples pr iteration.\n",
    "* See how the model degenerates if fed to much incestious training data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# semisupervised learning class here.\n",
    "############## ADVICE ################\n",
    "# Experiment with the implementation #\n",
    "# outside of the class object.       #\n",
    "######################################\n",
    "\n",
    "def semi_supervised(object): \n",
    "    def __init__(self,certainty_threshold=0.9,k_samples=1,unlabeled_x,unlabed_y):\n",
    "        # level of certainty acceptable to use for training\n",
    "        self.certainty_threshold = certainty_threshold # New predicted training examples will end if they fall below this certainty\n",
    "        # number of samples pr iteration.\n",
    "        self.k_samples\n",
    "        # Unlabeled data\n",
    "        self.unlabeled_x = unlabeled_x\n",
    "        self.unlabed_y = unlabeled_y\n",
    "        self.labeled_x = []\n",
    "        self.labeled_y = []\n",
    "        \n",
    "    def initialize_classifier(self,clf,x_train,y_train):\n",
    "        self.clf = clf\n",
    "        # fit classifier\n",
    "        self.clf.fit(x_train,y_train)\n",
    "        # and initialize container for predicted labels\n",
    "        self.labeled_x = x_train\n",
    "        self.labeled_y = y_train\n",
    "        \n",
    "    def partial_fit(self):\n",
    "        \"\"\"This function will fit the new training_data\"\"\"\n",
    "    def get_most_certain(self):\n",
    "        \"\"\"This function will get the next K predicted labels based on the parameter k_samples, unless the certainty threshold\n",
    "        Return | False if non found, otherwise adds the new samples to the labeled_x, and labeled_y, pops the samples from the unlabed x and y, and returns True\"\"\"\n",
    "\n",
    "    def fit(self,initialize):\n",
    "        \"\"\"This function will run all iterations, until no new samples with certainty can be found \"\"\"\n",
    "        \n",
    "    def predict(x_test):\n",
    "        \"This function will do the inference and return the argmax of P(x|C_i). Also described in Mannings book.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning(1)\n",
    "Here I want you to experiment with auxilliary tasks.\n",
    "\n",
    "**ex.2.1**\n",
    "* Make a test-setup where you train a classifiers two distinquish between all 90 classes in the reuters dataset (features you decide yourself), and then transform all documents to a vector that represent each of these classifiers output.\n",
    "\n",
    "**ex.2.2**\n",
    "* Use this feature space to predict classes each class.\n",
    "* Combine this feature space with the BoW representation (might have to convert to dense matrix - i.e. mat.to_dense(), before you can add the columns), or the doc2vec representation, and see the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked learning(2)\n",
    "Here I want you to combine the embedding features of each document with the BoW to see if a classifier might gain from having both inputs.\n",
    "\n",
    "**ex3.1**\n",
    "Investigate if combining the Doc2Vec features with BoW features will help a classifer.\n",
    "**Ex3.2** \n",
    "Add the doc2vec classifiers output as an extra feature to the BoW input (might have to convert the Sparse BoW representation to a dense matrix). And train an SVM classifier. \n",
    "* Compare the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**extra** -- or just inspiration for a theoretical Project.\n",
    "* Investigate if: Embedding meta-data (books, rating,year), will help with the classifier, and the domain adaptation (i.e. testing the performance of models trained on data from one domain (e.g. books), on another domaain(e.g. electronics).\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "105px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

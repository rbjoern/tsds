{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning 3\n",
    "\n",
    "Econometrics and machine learning\n",
    "\n",
    "**Andreas Bjerre-Nielsen**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Plan\n",
    "\n",
    "- 13-14: Lecture on machine learning 3\n",
    "- 14-15: Lecture by David D. Lassen \n",
    "- Work on assignment 2 + project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "How to use **machine learning** (*ML*) as a social scientist?\n",
    "- Direct applications\n",
    "- Prediction policy problems\n",
    "- Machine learning in estimation\n",
    "  - Instrumental variables\n",
    "  - Matching with machine learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Recap \n",
    "\n",
    "What are some strengths of ML?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Better predictions out-of-sample (lower error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we do it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Model selected from data + handle non-linearities\n",
    "- Cross validation, ensemble learning, regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Direct applications of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct applications of ML  (1): testing\n",
    "\n",
    "ML helps us with making predictive models: \n",
    "\n",
    "- Assess the performance of our models\n",
    "- Choose the parameters that help estimate the best performing model \n",
    "\n",
    "Can we use ML to help us clarify whether a new feature set is relevant for prediction?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct applications of ML  (2): testing\n",
    "\n",
    "Using a ML framework we test whether a model's performance is affected by inclusion of a feature set or not. \n",
    "\n",
    "- We can bootstrap standard error of performance and make test.\n",
    "\n",
    "Examples of implementation:\n",
    "\n",
    "- Moritz, B. and Zimmermann, T., 2016. Tree-based conditional portfolio sorts: The relation between past and future stock returns.\n",
    "\n",
    "- Bjerre-Nielsen, A. et al., 2018: Wi-Finding: Urban Transportation Sensing \n",
    "Using Crowdsourced Wi-Fi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct applications of ML  (3): testing\n",
    "\n",
    "How does this work? \n",
    "\n",
    "- Make models with and without feature set and compare difference in performance\n",
    "- We can bootstrap standard error of difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct applications of ML  (4): new data\n",
    "\n",
    "Can machine learning help us get new data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we do not know the socioeconomic composition of a neighborhood. Could machine learning help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Naik, Raskar, Hidalgo (2016): Cities Are Physical Too: Using Computer Vision to Measure the Quality and Impact of Urban Appearance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct applications of ML (5): new data\n",
    "\n",
    "Machine learning can help us *'fill in the blanks'* and impute missing data\n",
    "\n",
    "Cell phone data\n",
    "- Inferring poverty.\n",
    "  - Blumenstock, Cadamuro, On (2015): Predicting Poverty and Wealth from Mobile Phone Metadata\n",
    "\n",
    "- Inferring mode of transportation.\n",
    "  - Redi et al. (2010): Using mobile phones to determine transportation modes\n",
    "  - Bjerre-Nielsen et al. (2018): Wi-Finding: Urban Transportation Sensing Using Crowdsourced Wi-Fi\n",
    "  \n",
    "- Sleep\n",
    "    - Cuttone et al. (2017): SensibleSleep: A Bayesian Model for Learning Sleep Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct applications of ML (6): new data\n",
    "\n",
    "Facebook data can help infer\n",
    "- Psychological profile, demographics (Cambridge Analytica)\n",
    "- Socioeconomic status \n",
    "  - Facebook has a new patent, see [here](https://www.cbinsights.com/research/facebook-patent-socioeconomic-detection/?utm_source=CB+Insights+Newsletter&utm_campaign=fa48df10a8-ThursNL_02_01_2018&utm_medium=email&utm_term=0_9dc0513989-fa48df10a8-89375681)\n",
    "- Other: voting, mood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction policy problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction policy problems (1)\n",
    "\n",
    "Social scientists are often involved in policies aimed at: \n",
    "- alleviating poverty, decrease drop-out, crime etc.\n",
    "\n",
    "Efficacy of these programs requires targetting of individuals:\n",
    "- who is most poor, who is most at risk of dropping out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction policy problems (2)\n",
    "\n",
    "Kleinberg et al. 2015 state the problem as: \n",
    "\n",
    "- choose $X_0$ to maximize welfare $\\pi(\\cdot)$\n",
    "- $\\pi$ is a function of:\n",
    "  - $y$ outcome variable which depend on policy in unknown way\n",
    "  - $X_0$ is the policy \n",
    "\n",
    "E.g. hiring an additional teacher - how does this affect average teacher ability? \n",
    "- Need to predict new teacher ability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction policy problems (3)\n",
    "\n",
    "We can derive optimal policy (total diff.):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\pi(X_0,y)}{dX_0}=\\frac{\\partial\\pi}{\\partial X_0}\\cdot\\underset{predict}{\\underbrace{Y}} + \\frac{\\partial\\pi}{\\partial Y} \\cdot \\underset{causal\\\\effect}{\\underbrace{ \\frac{\\partial Y}{\\partial X_0}}}\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction policy problems (4)\n",
    "\n",
    "Example: joint knee and hip surgery. \n",
    "\n",
    "- many patients die shortly after surgery\n",
    "- predict mortality risk\n",
    "    - top 1 pct. riskiest: 44 pct. mortality rate, \\$30M saved\n",
    "    - top 5 pct. riskiest: 35 pct. mortality rate, \\$121M saved\n",
    "    - top 10 pct. riskiest: 24 pct. mortality rate, \\$158M saved\n",
    "    - top 20 pct. riskiest: 15 pct. mortality rate, \\$185M saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction policy problems (5)\n",
    "\n",
    "Other issues:\n",
    "- discrimination?\n",
    "    - gender, race, socio-economic status\n",
    "- GPDR: profiling\n",
    "- faith in selection algorithm?\n",
    "- should we incentivize local authorities to use private information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning in estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML in econometric methods\n",
    "\n",
    "Overview of econometric tools\n",
    "\n",
    "- instrumental variable\n",
    "- matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instrument variables and machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrument variable (1)\n",
    "\n",
    "Standard problem\n",
    "- We are interested in effect $X$ on $Y$\n",
    "    - Model $Y = \\beta X + u$ \n",
    "    - However, the two are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrument variable (2)\n",
    "\n",
    "Linear two step approach:\n",
    "- 1st stage: predict $X$ from $Z$, call this $\\hat{X}$\n",
    "    - standard approach: regress covariate $X$ on exogenous instruments $Z$\n",
    "    - often requires i) $Z$,$u$ are uncorrelated, ii) $Z$,$X$ are correlated \n",
    "- 2nd stage:\n",
    "    - regress $Y$ on $\\hat{X}$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrument variable (3)\n",
    "\n",
    "The issue:\n",
    "- 1st stage prediction of $\\hat{X}$ may have (very) poor fit\n",
    "- causes: \n",
    "    - sample size is low\n",
    "    - the number of instruments is high\n",
    "    - or the instruments are weak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrument variable (4)\n",
    "\n",
    "Cross validation solutions\n",
    "- Split sample (training, estimation)\n",
    "    - Angrist and Krueger 1995\n",
    "- Jack-knife  (leave one out)\n",
    "    - Angrist, Imbens, and Krueger 1999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrument variable (5)\n",
    "\n",
    "ML enhanced solutions\n",
    "- Regularization:    \n",
    "    - LASSO: Belloni, Chen, Chernozhukov, and Hansen 2012\n",
    "    - Ridge regression: Carrasco, 2012; Hansen and Kozbur 2014. \n",
    "- Neural network:\n",
    "    - Hartford, Leyton-Brown, and Taddy 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometric matching and machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matching and treatment (1)\n",
    "\n",
    "Aim understand policy treatment D on outcome Y:\n",
    "\n",
    "- Variable of interest (often called *treatment*): $D_i$\n",
    "\n",
    "- Outcome of interest: $Y_i$\n",
    "\n",
    "**Potential outcome framework**\n",
    "$$\n",
    "Y_i = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "Y_{1i} & \\text{if } D_i = 1,\\\\\n",
    "Y_{0i} & \\text{if } D_i = 0\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "The observed outcome $Y_i$ can be written in terms of potential outcomes as\n",
    "$$ Y_i = Y_{0i} + (Y_{1i}-Y_{0i})D_i$$\n",
    "\n",
    "$Y_{1i}-Y_{0i}$ is the *causal* effect of $D_i$ on $Y_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matching and treatment (2)\n",
    "\n",
    "Can we measure causal effect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problem: We never observe the same individual $i$ in both states. This is the **fundamental problem of causal inference**, Holland, 1986. Implication: cannot take difference within individual across states.\n",
    "\n",
    "Need to estimate the state we do not observe (the ***counterfactual***). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can we use a naive comparison of averages by treatment status? i.e. $E[Y_i|D_i = 1] - E[Y_i|D_i = 0]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes, if **random** assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matching and treatment (3)\n",
    "\n",
    "Suppose not random assignment. We can decompose into:\n",
    "\n",
    "\\begin{align}E[Y_i|D_i = 1] - E[Y_i|D_i = 0] =&  \n",
    "\\underset{causal\\,effect}{\\underbrace{E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1]}} + \\underset{selection\\,bias}{\\underbrace{E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0]}}\\end{align}\n",
    "\n",
    "The decomposition:\n",
    "\n",
    " - $E[Y_{1i}|D_i = 1] - E[Y_{0i}|D_i = 1] = E[Y_{1i} - Y_{0i}|D_i = 1]$: the average *causal* effect of $D_i$ on $Y$. \n",
    "\n",
    "- $E[Y_{0i}|D_i = 1] - E[Y_{0i}|D_i = 0]$: difference in average $Y_{0i}$ between the two groups. Likely to be different from 0 when individuals are allowed to self-select into treatment. Often referred to as ***selection bias***. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matching and treatment (4)\n",
    "\n",
    "Selection bias can be overcome by **matching**: find similar observations to construct counter factual\n",
    "\n",
    "- propensity score (inferred treatment assignment probability)\n",
    "- exact / nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matching and treatment (5)\n",
    "\n",
    "ML can help to estimate propensity score:\n",
    "\n",
    "\n",
    "- Using cross validation and various models including random forest:\n",
    "    - Lee, Lessler, and Stuart (2010) use ML\n",
    "- Debiased machine learning:\n",
    "    - Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (forthcoming)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matching and treatment (6)\n",
    "\n",
    "When we have random data ML can help us estimate heterogeneous treatment effects:\n",
    "\n",
    "- Causal Trees:\n",
    "    - Athey Imbens (2016)\n",
    "- Causal Forests:\n",
    "    - Athey, Tibshirani, Wager (forthcoming)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
